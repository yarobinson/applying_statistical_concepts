{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages and Import Dataset\n",
    "\n",
    "In this notebook, we’ll be working with the **Wisconsin Diagnostic Breast Cancer (WDBC)** dataset again. As a reminder, this dataset contains several features derived from digitized images of breast tumor cells obtained via fine needle aspirates (FNAs).\n",
    "\n",
    "Each row in the dataset represents one tumor, with a set of measurements calculated from the cell nuclei present in the image. The dataset has the following columns:\n",
    "- **ID**: Unique identifier for each tumor sample.\n",
    "- **Diagnosis**: The classification label for the tumor (Malignant or Benign).\n",
    "- **Radius Mean, Texture Mean, Perimeter Mean, Area Mean**: Various statistical properties of the tumor.\n",
    "- **Compactness, Concavity, Symmetry**: Other characteristics calculated from the shape and structure of the tumor cells.\n",
    "\n",
    "The target column is **Diagnosis**, which we will try to predict based on the other features in the dataset.\n",
    "\n",
    "This dataset was obtained from the UCI Machine Learning Repository, a well-known resource for datasets in the machine learning community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('dataset/wdbc.csv')\n",
    "cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean data \n",
    "by renaming \"M\" to \"Malignant\" and \"B\" to \"Benign\" using the `.replace` method. The `.replace` method takes one argument: a dictionary that maps previous values to desired new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer[\"diagnosis\"] = cancer[\"diagnosis\"].replace({\n",
    "    \"M\" : \"Malignant\",\n",
    "    \"B\" : \"Benign\"\n",
    "})\n",
    "\n",
    "cancer[\"diagnosis\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the `scikit-learn package` helps both with building a classifier as well as evaluating its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Golden rule of machine learning**: you cannot use the test data to build the model! If you do, the model gets to “see” the test data in advance, making it look more accurate than it really is. \n",
    "\n",
    "How about simplifying it like this:\n",
    "\n",
    "*Imagine how dangerous it could be if your model incorrectly predicts a patient’s tumor is benign when it’s actually malignant. Overestimating the accuracy of your model could lead to serious mistakes like that.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, when splitting data for classification tasks, we divide it into a training set and a test set to evaluate the model's performance. The training set, which usually makes up 50% to 95% of the total data, is used to help the model learn patterns and relationships. The remaining 5% to 50% is set aside as the test set, which is used to assess how well the model can handle new, unseen data.\n",
    "\n",
    "Think of it like preparing a doctor for an exam where they need to decide whether a tumor is cancerous or not. The training set is like the doctor’s study material—cases they practice with to get familiar with the symptoms and patterns of cancerous tumors. But you wouldn’t want to give them all the cases upfront. You need to reserve some unseen cases for the real test, which is where the test set comes in. These are the new cases the doctor faces during the exam, and the goal is to see how well they can apply their learning to fresh examples. If they perform well on the test cases, it shows that they’ve truly understood the task and aren’t just memorizing the practice cases.\n",
    "\n",
    "In practice, a 75/25 or 80/20 split is common for classification tasks, meaning 75% of the data is used for training and 25% for testing. This balance ensures the model has enough data to learn effectively, while still reserving a significant portion to evaluate how well it performs on new, unseen data. This approach helps prevent overfitting, where the model performs well on the training data but struggles with real-world examples.\n",
    "\n",
    "For this example, we’ll use 75% of the data for training—so the model has enough information to learn patterns—and 25% for testing, to ensure we have a reliable measure of its performance on unseen cases.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` function from scikit-learn simplifies data splitting. Key parameters to consider are:\n",
    "\n",
    "- **`shuffle=True`** (default): Shuffles the data before splitting to avoid any ordering effects.\n",
    "- **`stratify`**: Ensures the training and testing sets maintain the same class distribution as the original data. For example, if 63% of your data is benign and 37% malignant, using `stratify` ensures these proportions are preserved in both sets.\n",
    "\n",
    "For reproducibility, we will set a random seed using numpy's `random.seed` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "#split the data\n",
    "cancer_train, cancer_test = train_test_split(\n",
    "    cancer, train_size=0.75, stratify=cancer[\"diagnosis\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` method shows that the training set has 426 observations and the test set has 143, reflecting a 75%/25% split. To check diagnosis distribution, use `value_counts(normalize=True)` on `cancer_train`. This reveals about 63% benign and 37% malignant cases, confirming that the class proportions were maintained in the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_train[\"diagnosis\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall Notebook 1 (Classification 1):\n",
    "\n",
    "Fit the model on the breast cancer data. \n",
    "\n",
    "The `X` argument specifies the predictor variables (independent variables), while the `y` argument specifies the response variable (dependent variable).\n",
    "\n",
    "Here, \n",
    "- `X=cancer_train[[\"perimeter_mean\", \"concavity_mean\"]]` *to specify both Perimeter and Concavity means are to be used as the predictors.* \n",
    "- `y=cancer_train[\"diagnosis\"]` *to specify that diagnosis is the response variable (the one we want to predict)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X=cancer_train[[\"perimeter_mean\", \"concavity_mean\"]], y=cancer_train[\"diagnosis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the K-nearest neighbors classifier, we can predict whether each case in the test set is cancerous or not. We then compare these predictions with the actual diagnoses. The `diagnosis` column shows the actual results, and the `predicted` column shows what our classifier predicted. We'll display only the `ID`, `diagnosis`, and `predicted` columns from the test set to assess the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_test[\"predicted\"] = knn.predict(cancer_test[[\"perimeter_mean\", \"concavity_mean\"]])\n",
    "cancer_test[[\"id\", \"diagnosis\", \"predicted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't know how good our predictions are, we need to measure their accuracy. One way to assess how well our predictions match the actual labels in the test set is by calculating the **prediction accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine accuracy using the `score` method. Here, we pass the test data (predictors) and the actual labels (`cancer_test[\"diagnosis\"]`) to the method. This will evaluate how well the classifier's predictions match the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(\n",
    "    cancer_test[[\"perimeter_mean\", \"concavity_mean\"]],\n",
    "    cancer_test[\"diagnosis\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the estimated accuracy of the classifier on the test data was 88%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is a simple and widely used measure to summarize a classifier's performance. However, it only shows how often the model is correct overall and doesn't provide insight into the types of errors it makes. To get a clearer picture, you can use a **confusion matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Confusion Matrix?\n",
    "\n",
    "A confusion matrix breaks down the number of correct and incorrect predictions for each class, revealing the specific kinds of mistakes the classifier tends to make.\n",
    "\n",
    "**True Positive:** A malignant observation that was classified as malignant (top left).\n",
    "\n",
    "**False Positive:** A benign observation that was classified as malignant (bottom left).\n",
    "\n",
    "**True Negative:** A benign observation that was classified as benign (bottom right).\n",
    "\n",
    "**False Negative:** A malignant observation that was classified as benign (top right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Predicted Malignant</th>\n",
    "    <th>Predicted Benign</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Actually Malignant</th>\n",
    "    <td>True Positive</td>\n",
    "    <td>False Negative</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Actually Benign</th>\n",
    "    <td>False Positive</td>\n",
    "    <td>True Negative</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the confusion matrix, we can use the `crosstab` function from pandas. Here, we provide the actual labels as the first argument and the predicted labels as the second argument. \n",
    "\n",
    "*Note* that crosstab orders columns alphabetically, so the positive label (Malignant) might not appear in the top left corner but will still be correctly labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    cancer_test[\"diagnosis\"],\n",
    "    cancer_test[\"predicted\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix reveals the following:\n",
    "\n",
    "- 42 observations were correctly predicted as malignant.\n",
    "- 84 observations were correctly predicted as benign.\n",
    "- 11 observations were incorrectly classified as benign when they were actually malignant.\n",
    "- 6 observations were incorrectly classified as malignant when they were actually benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a perfect world, the classifier would have no false negatives or false positives and would achieve 100% accuracy. **In practice**, errors are inevitable. Therefore, consider which types of errors matter most for your application and use the confusion matrix to measure them. Key metrics derived from the confusion matrix include **precision** and **recall**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision measures how many of the predicted positives are actually positive. High precision means that when the classifier predicts a positive, it's likely to be correct.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\textbf{False Positives}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall measures how many actual positive observations were correctly identified by the classifier. High recall means that if there is a positive instance in the test data, the classifier is likely to detect it.\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\textbf{False Negatives}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually coding the math, you can use the `precision_score` and `recall_score` functions from scikit-learn to compute precision and recall. \n",
    "\n",
    "Set the following parameters:\n",
    "- `y_true`: Actual labels from the diagnosis variable.\n",
    "- `y_pred`: Predicted labels from the predicted variable.\n",
    "- `pos_label`: Specify the label to be considered positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(\n",
    "    y_true=cancer_test[\"diagnosis\"],\n",
    "    y_pred=cancer_test[\"predicted\"],\n",
    "    pos_label=\"Malignant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision calculation by hand:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (42)}}{\\text{True Positives (42)} + \\text{False Positives (6)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(\n",
    "    y_true=cancer_test[\"diagnosis\"],\n",
    "    y_pred=cancer_test[\"predicted\"],\n",
    "    pos_label=\"Malignant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall calculation by hand:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives (42)}}{\\text{True Positives (42)} + \\text{False Negatives (11)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the classifier achieved an estimated precision of 87.5% and a recall of 79% on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most predictive models in statistics and machine learning have parameters that influence their behavior. For instance, in K-nearest neighbors classification, $k$ is a parameter that determines how many neighbors contribute to the class vote. Different values of $k$ result in different classifiers and predictions.\n",
    "\n",
    "To find the best value for $k$ or tune any model parameter, we aim to maximize the classifier’s accuracy on unseen data. However, the test set should not be used during tuning. Instead, we split the training data into two subsets: one for **training the model** and the other for **evaluating its performance (validation)**. This approach helps select the optimal parameter value while keeping the test set untouched.\n",
    "\n",
    "so the data split would look like:\n",
    "\n",
    "![](./images/TVT.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we’ll start by creating a single 75-25 train/validation split, training a K-nearest neighbors model, and evaluating its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're re-using the train_test_split function here in order to split the training data into sub-training and validation sets.\n",
    "cancer_subtrain, cancer_validation = train_test_split(\n",
    "    cancer_train, train_size=0.75, stratify=cancer_train[\"diagnosis\"]\n",
    ")\n",
    "\n",
    "# fit the model on the sub-training data\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "X = cancer_subtrain[[\"perimeter_mean\", \"concavity_mean\"]]\n",
    "y = cancer_subtrain[\"diagnosis\"]\n",
    "knn.fit(X, y)\n",
    "\n",
    "# compute the score on validation data\n",
    "acc = knn.score(\n",
    "    cancer_validation[[\"perimeter_mean\", \"concavity_mean\"]],\n",
    "    cancer_validation[\"diagnosis\"]\n",
    ")\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we repeat the above code 4 more times, each time generating a new split with a different shuffle of the data, we get five different train/validation splits. \n",
    "\n",
    "![](./images/random_folds.001.png)\n",
    "\n",
    "Each split produces a new accuracy value based on the specific training and validation subsets used. This results in five different accuracy estimates over different runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of random splits, we can use **cross-validation** to ensure each observation is in the validation set only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "In cross-validation, we divide the training data into $C$ equal parts. Each part is used once as the validation set while the other $C-1$ parts form the training set. \n",
    "\n",
    "So as an example lets try and perform a 5 fold cross-validation.\n",
    "\n",
    "![](./images/CV.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform 5-fold cross-validation in Python using scikit-learn, use the `cross_validate` function. Here’s how:\n",
    "\n",
    "1. **Set `cv` to 5** for 5 folds.\n",
    "2. **Provide the training data predictors and labels** as `X` and `y`.\n",
    "\n",
    "The `cross_validate` function returns a dictionary. Convert it to a pandas DataFrame with `pd.DataFrame` for better visualization. It also automatically handles class stratification in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "X = cancer_train[[\"perimeter_mean\", \"concavity_mean\"]]\n",
    "y = cancer_train[\"diagnosis\"]\n",
    "\n",
    "returned_dictionary = cross_validate(\n",
    "    estimator=knn,\n",
    "    cv=5,    # setting up the cross validation number\n",
    "    X=X,\n",
    "    y=y\n",
    ")\n",
    "\n",
    "cv_5_df = pd.DataFrame(returned_dictionary)    # Converting it to pandas DataFrame\n",
    "\n",
    "cv_5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard error of the mean (SEM) for each column\n",
    "cv_5_metrics = cv_5_df.agg([\"mean\", \"sem\"])\n",
    "\n",
    "cv_5_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation scores are in the `test_score` column. To summarize the classifier’s performance:\n",
    "\n",
    "- **Mean (mean)**: Represents the estimated accuracy.\n",
    "- **Standard Error (sem)**: Indicates the uncertainty around this estimate.\n",
    "\n",
    "For example, if the mean accuracy is 0.863 and the standard error is 0.019, the true average accuracy is likely between 84.4% and 88.2%, though it could be outside this range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now what?\n",
    "\n",
    "To recap, the goal here is to see how accurate our model can be and figure out the best way to make it better. In this case, we still have one parameter we can tweak: the number of neighbors, $K$.\n",
    "\n",
    "To find the best value for $k$ using cross-validation, we can use `GridSearchCV` from Scikit-learn. It automates the process of trying out different values for $k$ and helps us find the one that gives the best accuracy.\n",
    "\n",
    "`GridSearchCV` will test various values for $k$ and select the one that yields the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">***Aside***: Number of Folds vs. Number of Neighbors (k)\n",
    ">\n",
    ">It’s important not to confuse the **number of folds** in cross-validation with the number of neighbors (k) in KNN.\n",
    ">\n",
    ">- **Number of Folds (Cross-Validation)**: Refers to how many parts we split the training data into for evaluation. For example, 5-fold cross-validation trains the model on 4 parts and tests it on the 5th, repeating the process. More folds give a better estimate of model performance but don’t affect the model’s behavior itself.\n",
    ">\n",
    ">- **Number of Neighbors (k in KNN)**: Refers to how many neighbors the model considers when making predictions. Smaller `k` values can lead to overfitting, while larger `k` values smooth predictions. Adjusting `k` directly improves model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `GridSearchCV` object:\n",
    "\n",
    "-  **Create the GridSearchCV object** by passing:\n",
    "   - `cancer_tune_pipe` as the `estimator`.\n",
    "   - `parameter_grid` as the `param_grid`.\n",
    "   - `cv=10` for 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The `range` function in Python generates sequences of numbers. \n",
    "\n",
    "`range(start, stop, step)` creates a sequence from `start` to `stop-1`, incrementing by `step`. For example, `range(1, 100, 5)` produces 1, 6, 11, ..., 96.\n",
    "`range(start, stop)` generates numbers from `start` to `stop-1`. For example, `range(1, 4)` produces 1, 2, 3.\n",
    "`range(stop)` starts from 0 and goes up to `stop-1`. For example, `range(4)` produces 0, 1, 2, 3.\n",
    "\"\"\"\n",
    "\n",
    "parameter_grid = {\n",
    "    \"n_neighbors\": range(1, 100, 5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_tune_grid = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=parameter_grid,\n",
    "    cv=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_tune_grid.fit(\n",
    "    cancer_train[[\"perimeter_mean\", \"concavity_mean\"]],\n",
    "    cancer_train[\"diagnosis\"]\n",
    ")\n",
    "\n",
    "accuracies_grid = pd.DataFrame(cancer_tune_grid.cv_results_)\n",
    "accuracies_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `GridSearchCV` results, focus on:\n",
    "\n",
    "- **Number of neighbors** (`param_n_neighbors`)\n",
    "- **Cross-validation accuracy estimate** (`mean_test_score`)\n",
    "- **Standard error of the accuracy estimate**\n",
    "\n",
    "GridSearchCV does not directly provide the standard error, but you can compute it using the standard deviation (`std_test_score`) with the formula:\n",
    "\n",
    "$$\n",
    " \\text{Standard Error} = \\frac{\\text{Standard Deviation}}{\\sqrt{\\text{Number of Folds}}} \n",
    " $$\n",
    "\n",
    "This formula allows you to estimate the uncertainty around the accuracy estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies_grid[\"sem_test_score\"] = accuracies_grid[\"std_test_score\"] / 10**(1/2)\n",
    "# accuracies_grid = (\n",
    "#     accuracies_grid[[\n",
    "#         \"param_n_neighbors\",\n",
    "#         \"mean_test_score\",\n",
    "#         \"sem_test_score\"\n",
    "#     ]]\n",
    "#     .rename(columns={\"n_neighbors\": \"param_n_neighbors\"})\n",
    "# )\n",
    "# accuracies_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decide which number of neighbors is best by plotting the accuracy versus $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot mean test scores with error bars\n",
    "plt.plot(accuracies_grid['param_n_neighbors'], accuracies_grid['mean_test_score'], '-o', color='blue')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy estimate')\n",
    "plt.title('K-Nearest Neighbors Performance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **also** obtain the number of neighbours with the highest accuracy programmatically by accessing the `best_params_ attribute` of the fit GridSearchCV object. \n",
    "\n",
    "*Note*  it is still useful to visualize the results as we did above since this provides additional information on how the model performance varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_tune_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing 66 neighbors gives the highest cross-validation accuracy estimate at 88.98%. \n",
    "\n",
    "Remember, these accuracy estimates are approximations. Even though 66 neighbors shows the highest accuracy here, it doesn't guarantee the classifier is truly better with this setting. When selecting parameters, aim for values where:\n",
    "\n",
    "- Accuracy is **roughly** optimal, suggesting the model will perform well.\n",
    "- Nearby values (e.g., slightly higher or lower) don't significantly decrease accuracy, ensuring reliability.\n",
    "- Training costs are manageable (e.g., avoid excessively large parameters that make predictions costly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "As you've seen in the above graph, something unusual happens as the number of neighbors ($k$) increases. The cross-validation accuracy might start to drop when $k$ gets too large. This is because the model becomes too general and starts to \"underfit\" the data. By testing a wider range of $k$ values with `GridSearchCV`, you can see how accuracy changes from small values of $k$ up to nearly the total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_param_grid = {\n",
    "    \"n_neighbors\": range(1, 385, 10),\n",
    "}\n",
    "\n",
    "large_cancer_tune_grid = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=large_param_grid,\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "large_cancer_tune_grid.fit(\n",
    "    cancer_train[[\"perimeter_mean\", \"concavity_mean\"]],\n",
    "    cancer_train[\"diagnosis\"]\n",
    ")\n",
    "\n",
    "large_accuracies_grid = pd.DataFrame(large_cancer_tune_grid.cv_results_)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot mean test scores with error bars\n",
    "plt.plot(large_accuracies_grid['param_n_neighbors'], large_accuracies_grid['mean_test_score'], '-o', color='blue')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy estimate')\n",
    "plt.title('K-Nearest Neighbors Performance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of neighbors $k$ increases, the classifier starts to average predictions over more distant points, smoothing the decision boundary and potentially leading to **underfitting**. This means the model becomes too simplistic and less sensitive to individual training examples, which might result in poor performance if the model doesn't capture the complexity of the data.\n",
    "\n",
    "Conversely, with a very small $k$, each data point has a stronger influence, making the decision boundary more jagged and sensitive to noise in the training data. This can lead to **overfitting**, where the model becomes too tailored to the training set and performs poorly on new, unseen data. In the extreme case where  $k$ is 1, the model simply matches new observations to their nearest training example, which can cause significant variability in predictions based on the training data used.\n",
    "\n",
    "📼 A video to better visualize underfitting and overfitting:\n",
    "\n",
    "[![](./images/Underfitting&Overfitting.png)](https://www.youtube.com/watch?v=o3DztvnfAJg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Split the Data**: Use `train_test_split` to divide the data into training and test sets. Set `stratify` to the class label column to maintain class distribution. Set the test set aside.\n",
    "\n",
    "2. **Define the Parameter Grid**: Specify the range of $k$ values to tune.\n",
    "\n",
    "3. **Perform Grid Search**: Use `GridSearchCV` with a parameter grid to estimate accuracy for different $k$ values.\n",
    "\n",
    "4. **Execute Grid Search**: Fit the `GridSearchCV` instance on the training data to find the best $k$.\n",
    "\n",
    "5. **Select Optimal $k$**: Choose the $k$ value with high accuracy and stable performance across nearby values.\n",
    "\n",
    "6. **Retrain the Model**: Create a new model with the best $k$ and fit it to the training data.\n",
    "\n",
    "7. **Evaluate the Model**: Assess the model's accuracy on the test set using the `score` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we worked through several steps to classify tumors as either benign or malignant using the Wisconsin Diagnostic Breast Cancer dataset and evaluate their performance. Here's a summary of what we covered:\n",
    "\n",
    "1. **K-Nearest Neighbors Algorithm (KNN):** We implemented the KNN algorithm and evaluated its performance on a test dataset.\n",
    "\n",
    "2. **Cross-Validation:** We used cross-validation to determine the optimal \n",
    "$k$ value for our classifier.\n",
    "\n",
    "3. **Underfitting/Overfitting**: We explored how varying $k$ can lead to underfitting or overfitting, discussing the implications of choosing a large or small $k$.\n",
    "\n",
    "We hope this notebook has provided a practical understanding of data classification, model evaluation, and the application of machine learning algorithms like KNN. Feel free to experiment further with the dataset or the code to enhance your learning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi",
   "language": "python",
   "name": "dsi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
